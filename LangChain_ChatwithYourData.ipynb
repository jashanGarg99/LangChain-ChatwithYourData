{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvKQu3T3fFvDQwkW3c9ouz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManthanVerma7/LangChain-ChatwithYourData/blob/main/LangChain_ChatwithYourData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEhcm_bKEudw"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# # Document Loading\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Note to students.\n",
        "# During periods of high load you may find the notebook unresponsive. It may appear to execute a cell, update the completion number in brackets [#] at the left of the cell but you may find the cell has not executed. This is particularly obvious on print statements when there is no output. If this happens, restart the kernel using the command under the Kernel tab.\n",
        "\n",
        "# ## Retrieval augmented generation\n",
        "#\n",
        "# In retrieval augmented generation (RAG), an LLM retrieves contextual documents from an external dataset as part of its execution.\n",
        "#\n",
        "# This is useful if we want to ask question about specific documents (e.g., our PDFs, a set of videos, etc).\n",
        "\n",
        "# ![overview.jpeg](attachment:overview.jpeg)\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "#! pip install langchain\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import os\n",
        "import openai\n",
        "import sys\n",
        "sys.path.append('../..')\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "\n",
        "# ## PDFs\n",
        "#\n",
        "# Let's load a PDF [transcript](https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf) from Andrew Ng's famous CS229 course! These documents are the result of automated transcription so words and sentences are sometimes split unexpectedly.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# The course will show the pip installs you would need to install packages on your own machine.\n",
        "# These packages are already installed on this platform and should not be run again.\n",
        "#! pip install pypdf\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "\n",
        "# Each page is a `Document`.\n",
        "#\n",
        "# A `Document` contains text (`page_content`) and `metadata`.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "len(pages)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "page = pages[0]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "print(page.page_content[0:500])\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "page.metadata\n",
        "\n",
        "\n",
        "# ## YouTube\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.document_loaders.generic import GenericLoader,  FileSystemBlobLoader\n",
        "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
        "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# ! pip install yt_dlp\n",
        "# ! pip install pydub\n",
        "\n",
        "\n",
        "# **Note**: This can take several minutes to complete. This has been modified relative to the lesson video to fetch the video file locally.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "url=\"https://www.youtube.com/watch?v=jGwO_UgTS7I\"\n",
        "save_dir=\"docs/youtube/\"\n",
        "loader = GenericLoader(\n",
        "    #YoutubeAudioLoader([url],save_dir),  # fetch from youtube\n",
        "    FileSystemBlobLoader(save_dir, glob=\"*.m4a\"),   #fetch locally\n",
        "    OpenAIWhisperParser()\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs[0].page_content[0:500]\n",
        "\n",
        "\n",
        "# ## URLs\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://github.com/basecamp/handbook/blob/master/titles-for-programmers.md\")\n",
        "\n",
        "\n",
        "# > Note: the URL sent to the WebBaseLoader differs from the one shonw in the video because for 2024 it was updated.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "print(docs[0].page_content[:500])\n",
        "\n",
        "\n",
        "# ## Notion\n",
        "\n",
        "# Follow steps [here](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/notion) for an example Notion site such as [this one](https://yolospace.notion.site/Blendle-s-Employee-Handbook-e31bff7da17346ee99f531087d8b133f):\n",
        "#\n",
        "# * Duplicate the page into your own Notion space and export as `Markdown / CSV`.\n",
        "# * Unzip it and save it as a folder that contains the markdown file for the Notion page.\n",
        "#\n",
        "\n",
        "# ![image.png](./img/image.png)\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.document_loaders import NotionDirectoryLoader\n",
        "loader = NotionDirectoryLoader(\"docs/Notion_DB\")\n",
        "docs = loader.load()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "print(docs[0].page_content[0:200])\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs[0].metadata\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# # Document Splitting\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import os\n",
        "import openai\n",
        "import sys\n",
        "sys.path.append('../..')\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "chunk_size =26\n",
        "chunk_overlap = 4\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap\n",
        ")\n",
        "c_splitter = CharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "\n",
        "# Why doesn't this split the string below?\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "text1 = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "r_splitter.split_text(text1)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "r_splitter.split_text(text2)\n",
        "\n",
        "\n",
        "# Ok, this splits the string but we have an overlap specified as 5, but it looks like 3? (try an even number)\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "r_splitter.split_text(text3)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "c_splitter.split_text(text3)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "c_splitter = CharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    separator = ' '\n",
        ")\n",
        "c_splitter.split_text(text3)\n",
        "\n",
        "\n",
        "# Try your own examples!\n",
        "\n",
        "# ## Recursive splitting details\n",
        "#\n",
        "# `RecursiveCharacterTextSplitter` is recommended for generic text.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
        "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
        "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
        "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
        "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
        "Sentences have a period at the end, but also, have a space.\\\n",
        "and words are separated by space.\"\"\"\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "len(some_text)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "c_splitter = CharacterTextSplitter(\n",
        "    chunk_size=450,\n",
        "    chunk_overlap=0,\n",
        "    separator = ' '\n",
        ")\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=450,\n",
        "    chunk_overlap=0,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "c_splitter.split_text(some_text)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "r_splitter.split_text(some_text)\n",
        "\n",
        "\n",
        "# Let's reduce the chunk size a bit and add a period to our separators:\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=150,\n",
        "    chunk_overlap=0,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",
        ")\n",
        "r_splitter.split_text(some_text)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=150,\n",
        "    chunk_overlap=0,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
        ")\n",
        "r_splitter.split_text(some_text)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs = text_splitter.split_documents(pages)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "len(docs)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "len(pages)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.document_loaders import NotionDirectoryLoader\n",
        "loader = NotionDirectoryLoader(\"docs/Notion_DB\")\n",
        "notion_db = loader.load()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs = text_splitter.split_documents(notion_db)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "len(notion_db)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "len(docs)\n",
        "\n",
        "\n",
        "# ## Token splitting\n",
        "#\n",
        "# We can also split on token count explicity, if we want.\n",
        "#\n",
        "# This can be useful because LLMs often have context windows designated in tokens.\n",
        "#\n",
        "# Tokens are often ~4 characters.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "text1 = \"foo bar bazzyfoo\"\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "text_splitter.split_text(text1)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs = text_splitter.split_documents(pages)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs[0]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "pages[0].metadata\n",
        "\n",
        "\n",
        "# ## Context aware splitting\n",
        "#\n",
        "# Chunking aims to keep text with common context together.\n",
        "#\n",
        "# A text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting.\n",
        "#\n",
        "# We can use `MarkdownHeaderTextSplitter` to preserve header metadata in our chunks, as show below.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.document_loaders import NotionDirectoryLoader\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "markdown_document = \"\"\"# Title\\n\\n \\\n",
        "## Chapter 1\\n\\n \\\n",
        "Hi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n",
        "### Section \\n\\n \\\n",
        "Hi this is Lance \\n\\n\n",
        "## Chapter 2\\n\\n \\\n",
        "Hi this is Molly\"\"\"\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on=headers_to_split_on\n",
        ")\n",
        "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "md_header_splits[0]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "md_header_splits[1]\n",
        "\n",
        "\n",
        "# Try on a real Markdown file, like a Notion database.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "loader = NotionDirectoryLoader(\"docs/Notion_DB\")\n",
        "docs = loader.load()\n",
        "txt = ' '.join([d.page_content for d in docs])\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "]\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on=headers_to_split_on\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "md_header_splits = markdown_splitter.split_text(txt)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "md_header_splits[0]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d00-R5fUE1pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# # Vectorstores and Embeddings\n",
        "#\n",
        "# Recall the overall workflow for retrieval augmented generation (RAG):\n",
        "\n",
        "# ![overview.jpeg](attachment:overview.jpeg)\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import os\n",
        "import openai\n",
        "import sys\n",
        "sys.path.append('../..')\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "\n",
        "# We just discussed `Document Loading` and `Splitting`.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load PDF\n",
        "loaders = [\n",
        "    # Duplicate documents on purpose - messy data\n",
        "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n",
        "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n",
        "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture02.pdf\"),\n",
        "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\")\n",
        "]\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 150\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "len(splits)\n",
        "\n",
        "\n",
        "# ## Embeddings\n",
        "#\n",
        "# Let's take our splits and embed them.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "sentence1 = \"i like dogs\"\n",
        "sentence2 = \"i like canines\"\n",
        "sentence3 = \"the weather is ugly outside\"\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "embedding1 = embedding.embed_query(sentence1)\n",
        "embedding2 = embedding.embed_query(sentence2)\n",
        "embedding3 = embedding.embed_query(sentence3)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "np.dot(embedding1, embedding2)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "np.dot(embedding1, embedding3)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "np.dot(embedding2, embedding3)\n",
        "\n",
        "\n",
        "# ## Vectorstores\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# ! pip install chromadb\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "persist_directory = 'docs/chroma/'\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "get_ipython().system('rm -rf ./docs/chroma  # remove old database files if any')\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "print(vectordb._collection.count())\n",
        "\n",
        "\n",
        "# ### Similarity Search\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"is there an email i can ask for help\"\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs = vectordb.similarity_search(question,k=3)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "len(docs)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs[0].page_content\n",
        "\n",
        "\n",
        "# Let's save this so we can use it later!\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "vectordb.persist()\n",
        "\n",
        "\n",
        "# ## Failure modes\n",
        "#\n",
        "# This seems great, and basic similarity search will get you 80% of the way there very easily.\n",
        "#\n",
        "# But there are some failure modes that can creep up.\n",
        "#\n",
        "# Here are some edge cases that can arise - we'll fix them in the next class.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"what did they say about matlab?\"\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs = vectordb.similarity_search(question,k=5)\n",
        "\n",
        "\n",
        "# Notice that we're getting duplicate chunks (because of the duplicate `MachineLearning-Lecture01.pdf` in the index).\n",
        "#\n",
        "# Semantic search fetches all similar documents, but does not enforce diversity.\n",
        "#\n",
        "# `docs[0]` and `docs[1]` are indentical.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs[0]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs[1]\n",
        "\n",
        "\n",
        "# We can see a new failure mode.\n",
        "#\n",
        "# The question below asks a question about the third lecture, but includes results from other lectures as well.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"what did they say about regression in the third lecture?\"\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs = vectordb.similarity_search(question,k=5)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "for doc in docs:\n",
        "    print(doc.metadata)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "print(docs[4].page_content)\n",
        "\n",
        "\n",
        "# Approaches discussed in the next lecture can be used to address both!\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zKRp0ntzFGxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# # Retrieval\n",
        "#\n",
        "# Retrieval is the centerpiece of our retrieval augmented generation (RAG) flow.\n",
        "#\n",
        "# Let's get our vectorDB from before.\n",
        "\n",
        "# ## Vectorstore retrieval\n",
        "#\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import os\n",
        "import openai\n",
        "import sys\n",
        "sys.path.append('../..')\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "#!pip install lark\n",
        "\n",
        "\n",
        "# ### Similarity Search\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "persist_directory = 'docs/chroma/'\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma(\n",
        "    persist_directory=persist_directory,\n",
        "    embedding_function=embedding\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "print(vectordb._collection.count())\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "texts = [\n",
        "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
        "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
        "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
        "]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "smalldb = Chroma.from_texts(texts, embedding=embedding)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "smalldb.similarity_search(question, k=2)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "smalldb.max_marginal_relevance_search(question,k=2, fetch_k=3)\n",
        "\n",
        "\n",
        "# ### Addressing Diversity: Maximum marginal relevance\n",
        "#\n",
        "# Last class we introduced one problem: how to enforce diversity in the search results.\n",
        "#\n",
        "# `Maximum marginal relevance` strives to achieve both relevance to the query *and diversity* among the results.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"what did they say about matlab?\"\n",
        "docs_ss = vectordb.similarity_search(question,k=3)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs_ss[0].page_content[:100]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs_ss[1].page_content[:100]\n",
        "\n",
        "\n",
        "# Note the difference in results with `MMR`.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs_mmr[0].page_content[:100]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs_mmr[1].page_content[:100]\n",
        "\n",
        "\n",
        "# ### Addressing Specificity: working with metadata\n",
        "#\n",
        "# In last lecture, we showed that a question about the third lecture can include results from other lectures as well.\n",
        "#\n",
        "# To address this, many vectorstores support operations on `metadata`.\n",
        "#\n",
        "# `metadata` provides context for each embedded chunk.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"what did they say about regression in the third lecture?\"\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs = vectordb.similarity_search(\n",
        "    question,\n",
        "    k=3,\n",
        "    filter={\"source\":\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\"}\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "for d in docs:\n",
        "    print(d.metadata)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ### Addressing Specificity: working with metadata using self-query retriever\n",
        "#\n",
        "# But we have an interesting challenge: we often want to infer the metadata from the query itself.\n",
        "#\n",
        "# To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
        "#\n",
        "# 1. The `query` string to use for vector search\n",
        "# 2. A metadata filter to pass in as well\n",
        "#\n",
        "# Most vector databases support metadata filters, so this doesn't require any new databases or indexes.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "metadata_field_info = [\n",
        "    AttributeInfo(\n",
        "        name=\"source\",\n",
        "        description=\"The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"page\",\n",
        "        description=\"The page from the lecture\",\n",
        "        type=\"integer\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "# **Note:** The default model for `OpenAI` (\"from langchain.llms import OpenAI\") is `text-davinci-003`. Due to the deprication of OpenAI's model `text-davinci-003` on 4 January 2024, you'll be using OpenAI's recommended replacement model `gpt-3.5-turbo-instruct` instead.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "document_content_description = \"Lecture notes\"\n",
        "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)\n",
        "retriever = SelfQueryRetriever.from_llm(\n",
        "    llm,\n",
        "    vectordb,\n",
        "    document_content_description,\n",
        "    metadata_field_info,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"what did they say about regression in the third lecture?\"\n",
        "\n",
        "\n",
        "# **You will receive a warning** about predict_and_parse being deprecated the first time you executing the next line. This can be safely ignored.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "for d in docs:\n",
        "    print(d.metadata)\n",
        "\n",
        "\n",
        "# ### Additional tricks: compression\n",
        "#\n",
        "# Another approach for improving the quality of retrieved docs is compression.\n",
        "#\n",
        "# Information most relevant to a query may be buried in a document with a lot of irrelevant text.\n",
        "#\n",
        "# Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
        "#\n",
        "# Contextual compression is meant to fix this.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def pretty_print_docs(docs):\n",
        "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Wrap our vectorstore\n",
        "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=vectordb.as_retriever()\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"what did they say about matlab?\"\n",
        "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
        "pretty_print_docs(compressed_docs)\n",
        "\n",
        "\n",
        "# ## Combining various techniques\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"what did they say about matlab?\"\n",
        "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
        "pretty_print_docs(compressed_docs)\n",
        "\n",
        "\n",
        "# ## Other types of retrieval\n",
        "#\n",
        "# It's worth noting that vectordb as not the only kind of tool to retrieve documents.\n",
        "#\n",
        "# The `LangChain` retriever abstraction includes other ways to retrieve documents, such as TF-IDF or SVM.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.retrievers import SVMRetriever\n",
        "from langchain.retrievers import TFIDFRetriever\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Load PDF\n",
        "loader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
        "pages = loader.load()\n",
        "all_page_text=[p.page_content for p in pages]\n",
        "joined_page_text=\" \".join(all_page_text)\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
        "splits = text_splitter.split_text(joined_page_text)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Retrieve\n",
        "svm_retriever = SVMRetriever.from_texts(splits,embedding)\n",
        "tfidf_retriever = TFIDFRetriever.from_texts(splits)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"What are major topics for this class?\"\n",
        "docs_svm=svm_retriever.get_relevant_documents(question)\n",
        "docs_svm[0]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"what did they say about matlab?\"\n",
        "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
        "docs_tfidf[0]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HnLkaNYcFWTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# # Question Answering\n",
        "\n",
        "# ## Overview\n",
        "#\n",
        "# Recall the overall workflow for retrieval augmented generation (RAG):\n",
        "\n",
        "# ![overview.jpeg](attachment:overview.jpeg)\n",
        "\n",
        "# We discussed `Document Loading` and `Splitting` as well as `Storage` and `Retrieval`.\n",
        "#\n",
        "# Let's load our vectorDB.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import os\n",
        "import openai\n",
        "import sys\n",
        "sys.path.append('../..')\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "\n",
        "# The code below was added to assign the openai LLM version filmed until it is deprecated, currently in Sept 2023.\n",
        "# LLM responses can often vary, but the responses may be significantly different when using a different model version.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import datetime\n",
        "current_date = datetime.datetime.now().date()\n",
        "if current_date < datetime.date(2023, 9, 2):\n",
        "    llm_name = \"gpt-3.5-turbo-0301\"\n",
        "else:\n",
        "    llm_name = \"gpt-3.5-turbo\"\n",
        "print(llm_name)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "persist_directory = 'docs/chroma/'\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "print(vectordb._collection.count())\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"What are major topics for this class?\"\n",
        "docs = vectordb.similarity_search(question,k=3)\n",
        "len(docs)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
        "\n",
        "\n",
        "# ### RetrievalQA chain\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever()\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "result = qa_chain({\"query\": question})\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "result[\"result\"]\n",
        "\n",
        "\n",
        "# ### Prompt\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"Is probability a class topic?\"\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "result = qa_chain({\"query\": question})\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "result[\"result\"]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "result[\"source_documents\"][0]\n",
        "\n",
        "\n",
        "# ### RetrievalQA chain types\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "qa_chain_mr = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"map_reduce\"\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "result = qa_chain_mr({\"query\": question})\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "result[\"result\"]\n",
        "\n",
        "\n",
        "# If you wish to experiment on the `LangSmith platform` (previously known as LangChain Plus):\n",
        "#\n",
        "#  * Go to [LangSmith](https://www.langchain.com/langsmith) and sign up\n",
        "#  * Create an API key from your account's settings\n",
        "#  * Use this API key in the code below\n",
        "#  * uncomment the code\n",
        "#  Note, the endpoint in the video differs from the one below. Use the one below.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "#import os\n",
        "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "#os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
        "#os.environ[\"LANGCHAIN_API_KEY\"] = \"...\" # replace dots with your api key\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "qa_chain_mr = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"map_reduce\"\n",
        ")\n",
        "result = qa_chain_mr({\"query\": question})\n",
        "result[\"result\"]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "qa_chain_mr = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"refine\"\n",
        ")\n",
        "result = qa_chain_mr({\"query\": question})\n",
        "result[\"result\"]\n",
        "\n",
        "\n",
        "# ### RetrievalQA limitations\n",
        "#\n",
        "# QA fails to preserve conversational history.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever()\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"Is probability a class topic?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"why are those prerequesites needed?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]\n",
        "\n",
        "\n",
        "# Note, The LLM response varies. Some responses **do** include a reference to probability which might be gleaned from referenced documents. The point is simply that the model does not have access to past questions or answers, this will be covered in the next section.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H6kVERATF3yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# # Chat\n",
        "#\n",
        "# Recall the overall workflow for retrieval augmented generation (RAG):\n",
        "\n",
        "# ![overview.jpeg](attachment:overview.jpeg)\n",
        "\n",
        "# We discussed `Document Loading` and `Splitting` as well as `Storage` and `Retrieval`.\n",
        "#\n",
        "# We then showed how `Retrieval` can be used for output generation in Q+A using `RetrievalQA` chain.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import os\n",
        "import openai\n",
        "import sys\n",
        "sys.path.append('../..')\n",
        "\n",
        "import panel as pn  # GUI\n",
        "pn.extension()\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "\n",
        "# The code below was added to assign the openai LLM version filmed until it is deprecated, currently in Sept 2023.\n",
        "# LLM responses can often vary, but the responses may be significantly different when using a different model version.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import datetime\n",
        "current_date = datetime.datetime.now().date()\n",
        "if current_date < datetime.date(2023, 9, 2):\n",
        "    llm_name = \"gpt-3.5-turbo-0301\"\n",
        "else:\n",
        "    llm_name = \"gpt-3.5-turbo\"\n",
        "print(llm_name)\n",
        "\n",
        "\n",
        "#  If you wish to experiment on the `LangSmith platform` (previously known as LangChain Plus):\n",
        "#\n",
        "#  * Go to [LangSmith](https://www.langchain.com/langsmith) and sign up\n",
        "#  * Create an api key from your account's settings\n",
        "#  * Use this api key in the code below\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "#import os\n",
        "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "#os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
        "#os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "persist_directory = 'docs/chroma/'\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"What are major topics for this class?\"\n",
        "docs = vectordb.similarity_search(question,k=3)\n",
        "len(docs)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
        "llm.predict(\"Hello world!\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Build prompt\n",
        "from langchain.prompts import PromptTemplate\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
        "\n",
        "# Run chain\n",
        "from langchain.chains import RetrievalQA\n",
        "question = \"Is probability a class topic?\"\n",
        "qa_chain = RetrievalQA.from_chain_type(llm,\n",
        "                                       retriever=vectordb.as_retriever(),\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
        "\n",
        "\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]\n",
        "\n",
        "\n",
        "# ### Memory\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "\n",
        "# ### ConversationalRetrievalChain\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "retriever=vectordb.as_retriever()\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"Is probability a class topic?\"\n",
        "result = qa({\"question\": question})\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "result['answer']\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "question = \"why are those prerequesites needed?\"\n",
        "result = qa({\"question\": question})\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "result['answer']\n",
        "\n",
        "\n",
        "# # Create a chatbot that works on your documents\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "# The chatbot code has been updated a bit since filming. The GUI appearance also varies depending on the platform it is running on.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def load_db(file, chain_type, k):\n",
        "    # load documents\n",
        "    loader = PyPDFLoader(file)\n",
        "    documents = loader.load()\n",
        "    # split documents\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    # define embedding\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    # create vector database from data\n",
        "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
        "    # define retriever\n",
        "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
        "    # create a chatbot chain. Memory is managed externally.\n",
        "    qa = ConversationalRetrievalChain.from_llm(\n",
        "        llm=ChatOpenAI(model_name=llm_name, temperature=0),\n",
        "        chain_type=chain_type,\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        return_generated_question=True,\n",
        "    )\n",
        "    return qa\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import panel as pn\n",
        "import param\n",
        "\n",
        "class cbfs(param.Parameterized):\n",
        "    chat_history = param.List([])\n",
        "    answer = param.String(\"\")\n",
        "    db_query  = param.String(\"\")\n",
        "    db_response = param.List([])\n",
        "\n",
        "    def __init__(self,  **params):\n",
        "        super(cbfs, self).__init__( **params)\n",
        "        self.panels = []\n",
        "        self.loaded_file = \"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"\n",
        "        self.qa = load_db(self.loaded_file,\"stuff\", 4)\n",
        "\n",
        "    def call_load_db(self, count):\n",
        "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
        "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
        "        else:\n",
        "            file_input.save(\"temp.pdf\")  # local copy\n",
        "            self.loaded_file = file_input.filename\n",
        "            button_load.button_style=\"outline\"\n",
        "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
        "            button_load.button_style=\"solid\"\n",
        "        self.clr_history()\n",
        "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
        "\n",
        "    def convchain(self, query):\n",
        "        if not query:\n",
        "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
        "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
        "        self.chat_history.extend([(query, result[\"answer\"])])\n",
        "        self.db_query = result[\"generated_question\"]\n",
        "        self.db_response = result[\"source_documents\"]\n",
        "        self.answer = result['answer']\n",
        "        self.panels.extend([\n",
        "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
        "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))\n",
        "        ])\n",
        "        inp.value = ''  #clears loading indicator when cleared\n",
        "        return pn.WidgetBox(*self.panels,scroll=True)\n",
        "\n",
        "    @param.depends('db_query ', )\n",
        "    def get_lquest(self):\n",
        "        if not self.db_query :\n",
        "            return pn.Column(\n",
        "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
        "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
        "            )\n",
        "        return pn.Column(\n",
        "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
        "            pn.pane.Str(self.db_query )\n",
        "        )\n",
        "\n",
        "    @param.depends('db_response', )\n",
        "    def get_sources(self):\n",
        "        if not self.db_response:\n",
        "            return\n",
        "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
        "        for doc in self.db_response:\n",
        "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
        "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
        "\n",
        "    @param.depends('convchain', 'clr_history')\n",
        "    def get_chats(self):\n",
        "        if not self.chat_history:\n",
        "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
        "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
        "        for exchange in self.chat_history:\n",
        "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
        "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
        "\n",
        "    def clr_history(self,count=0):\n",
        "        self.chat_history = []\n",
        "        return\n",
        "\n",
        "\n",
        "# ### Create a chatbot\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "cb = cbfs()\n",
        "\n",
        "file_input = pn.widgets.FileInput(accept='.pdf')\n",
        "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
        "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
        "button_clearhistory.on_click(cb.clr_history)\n",
        "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
        "\n",
        "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
        "conversation = pn.bind(cb.convchain, inp)\n",
        "\n",
        "jpg_pane = pn.pane.Image( './img/convchain.jpg')\n",
        "\n",
        "tab1 = pn.Column(\n",
        "    pn.Row(inp),\n",
        "    pn.layout.Divider(),\n",
        "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
        "    pn.layout.Divider(),\n",
        ")\n",
        "tab2= pn.Column(\n",
        "    pn.panel(cb.get_lquest),\n",
        "    pn.layout.Divider(),\n",
        "    pn.panel(cb.get_sources ),\n",
        ")\n",
        "tab3= pn.Column(\n",
        "    pn.panel(cb.get_chats),\n",
        "    pn.layout.Divider(),\n",
        ")\n",
        "tab4=pn.Column(\n",
        "    pn.Row( file_input, button_load, bound_button_load),\n",
        "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
        "    pn.layout.Divider(),\n",
        "    pn.Row(jpg_pane.clone(width=400))\n",
        ")\n",
        "dashboard = pn.Column(\n",
        "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
        "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
        ")\n",
        "dashboard\n",
        "\n",
        "\n",
        "# Feel free to copy this code and modify it to add your own features. You can try alternate memory and retriever models by changing the configuration in `load_db` function and the `convchain` method. [Panel](https://panel.holoviz.org/) and [Param](https://param.holoviz.org/) have many useful features and widgets you can use to extend the GUI.\n",
        "#\n",
        "\n",
        "# ## Acknowledgments\n",
        "#\n",
        "# Panel based chatbot inspired by Sophia Yang, [github](https://github.com/sophiamyang/tutorials-LangChain)\n"
      ],
      "metadata": {
        "id": "JqYlqNe8GA_t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}